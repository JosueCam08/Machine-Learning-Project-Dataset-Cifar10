{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxoMpbKyvU0P",
        "outputId": "bb8141c3-52a3-4167-824c-288d26eec7aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cache in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Python 3.7.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.48.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n"
          ]
        }
      ],
      "source": [
        "# Borrar al final\n",
        "!pip install cache\n",
        "!python --version\n",
        "!pip install tensorflow==1.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbVWocPx6wp3",
        "outputId": "e09bc839-a9ba-4a5a-d285-0bbc85d06760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ]
        }
      ],
      "source": [
        "# Librerias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf # Funciones de redes neuronales\n",
        "%matplotlib inline\n",
        "import cifar10 # Base de datos de imagenes para la parte de entrenamiento\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqlt0UNFuoQw",
        "outputId": "7fecdc3a-efcd-46a0-c793-f08e99707a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
            "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n"
          ]
        }
      ],
      "source": [
        "from utils import load_cifar10_data # Utils se encarga de descargar los datos de entrenamiento\n",
        "                                    # y separarlos para el entrenamiento\n",
        "x_train, y_train, x_validate, y_validate, x_test, y_test = load_cifar10_data()\n",
        "# x - Datos / y - etiquetas\n",
        "# Train - Los datos de entrenamiento\n",
        "# Validate - Paso intermedio para comprobar el proceso de entrenamiento\n",
        "# Test- Prueba final para comprobar el aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCViUwYFwoZF",
        "outputId": "8dec4978-7a78-4002-9796-8ab64b15c6ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/network.py:15: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import network, train, utils\n",
        "from layers import ReluLayer, BinaryFullyConnectedLayer, \\\n",
        "    BinaryConvolutionLayer, BatchNormLayer, MaxPoolingLayer\n",
        "\n",
        "# Netwrok - Define la red neuronal y sus funciones correspondientes\n",
        "# Train - Algoritmo de MSA que realiza el entrenamiento\n",
        "# Layers - Abstracion de las capas, los pesos asignados, funciones forward y backward, funcion Hamiltoniano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpLQoPeHyoGJ",
        "outputId": "683947ec-0e8e-43aa-ce8c-ea480a31dbaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/network.py:35: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/network.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/layers.py:411: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/layers.py:415: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/layers.py:446: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/layers.py:173: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/layers.py:294: The name tf.assign_sub is deprecated. Please use tf.compat.v1.assign_sub instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/layers.py:300: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/layers.py:561: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/network.py:89: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/network.py:89: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/network.py:90: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/network.py:91: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Red neuronal con las caracteristicas | Input nodos, output nodos, loss function\n",
        "nn = network.NeuralNetwork(in_size=[None, 32, 32, 3], n_out_classes=10, \n",
        "                           loss_func=utils.smooth_hinge_loss)\n",
        "nn.reset_graph()\n",
        "\n",
        "# Capas que estan enmedio de la red neuronal \n",
        "# Hidden Conv-1\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=128, filter_size=3))\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Conv-2\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=128, filter_size=3))\n",
        "nn.add_layer(MaxPoolingLayer())\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Conv-3\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=256, filter_size=3))\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Conv-4\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=256, filter_size=3))\n",
        "nn.add_layer(MaxPoolingLayer())\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Conv-5\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=512, filter_size=3))\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Conv-6\n",
        "nn.add_layer(BinaryConvolutionLayer(\n",
        "    out_dim=512, filter_size=3))\n",
        "nn.add_layer(MaxPoolingLayer())\n",
        "nn.add_layer(BatchNormLayer(axes=[0, 1, 2]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Fc-7\n",
        "nn.add_layer(BinaryFullyConnectedLayer(\n",
        "    out_dim=1024))\n",
        "nn.add_layer(BatchNormLayer(axes=[0]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Fc-8\n",
        "nn.add_layer(BinaryFullyConnectedLayer(\n",
        "    out_dim=1024))\n",
        "nn.add_layer(BatchNormLayer(axes=[0]))\n",
        "nn.add_layer(ReluLayer())\n",
        "\n",
        "# Hidden Fc-9\n",
        "nn.add_layer(BinaryFullyConnectedLayer(out_dim=10))\n",
        "nn.add_layer(BatchNormLayer(axes=[0]))\n",
        "\n",
        "nn.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKc7tMND6Qnc",
        "outputId": "808a0fa5-18f6-4581-c0c6-d5678ce3fe54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/train.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define el vector de entrenamiento\n",
        "data_train = (x_train, y_train)\n",
        "opt = train.Trainer(nn, data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJAVZQ2y6kvQ"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters | Tasa de penalizacion & Promedio de error\n",
        "opt.set_rho(0.5)\n",
        "opt.set_ema_rates(0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYJvko8M6rxu",
        "outputId": "924682b9-462f-449a-f080-1ad793f6ce05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "========= Begin epoch =========\n",
            "batch_size = 100\n",
            "EMA rates:\n",
            "[0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999, 0.999]\n",
            "rho:\n",
            "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
            "Iter: 0 of 450 || Estimated train loss/acc: 1.157259, 0.24\n",
            "Iter: 45 of 450 || Estimated train loss/acc: 1.176455, 0.21\n",
            "Iter: 90 of 450 || Estimated train loss/acc: 1.064445, 0.21\n",
            "Iter: 135 of 450 || Estimated train loss/acc: 0.950343, 0.23\n",
            "Iter: 180 of 450 || Estimated train loss/acc: 0.944341, 0.19\n",
            "Iter: 225 of 450 || Estimated train loss/acc: 0.893873, 0.27\n",
            "Iter: 270 of 450 || Estimated train loss/acc: 0.874758, 0.14\n",
            "Iter: 315 of 450 || Estimated train loss/acc: 0.891537, 0.17\n",
            "Iter: 360 of 450 || Estimated train loss/acc: 0.760601, 0.26\n",
            "Iter: 405 of 450 || Estimated train loss/acc: 0.694233, 0.23\n"
          ]
        }
      ],
      "source": [
        "losses_and_accs_train = []\n",
        "losses_and_accs_valid = []\n",
        "losses_and_accs_test = []\n",
        "\n",
        "# Numero de entrenamientos\n",
        "n_epochs = 500\n",
        "\n",
        "for t in range(n_epochs):        \n",
        "    print('Epoch: ', t)\n",
        "\n",
        "    # Tamanio de datos | Tasa de aprendizaje es dinamico | Nodos outputs\n",
        "    opt.train_epoch(batch_size=100, ema_decay=0.98, n_output=10, verbose=True)\n",
        "    losses_and_accs_train.append(\n",
        "        opt.loss_and_accuracy((x_train, y_train), max_batch=400, inference=True))\n",
        "    losses_and_accs_test.append(\n",
        "        opt.loss_and_accuracy((x_test, y_test), max_batch=400, inference=True))\n",
        "    losses_and_accs_valid.append(\n",
        "        opt.loss_and_accuracy((x_validate, y_validate), max_batch=400, inference=True))\n",
        "\n",
        "# Presenta valor de perdida y el porcentaje de exactitud\n",
        "    print('Train loss/acc: ', losses_and_accs_train[-1],\n",
        "          'Test loss/acc: ', losses_and_accs_test[-1])\n",
        "  \n",
        "losses_and_accs_train = np.asarray(losses_and_accs_train)\n",
        "losses_and_accs_valid = np.asarray(losses_and_accs_valid)\n",
        "losses_and_accs_test = np.asarray(losses_and_accs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "6LdxMyRcgkkS",
        "outputId": "86dfdaaa-8345-47d5-f9c1-657ea0bb1775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  (0.8120519351959229, 0.3922222220897675)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3e0ac037c529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                        max_batch=400))\n\u001b[1;32m      3\u001b[0m print('Valid: ', opt.loss_and_accuracy((x_validate, y_validate), inference=True,\n\u001b[0;32m----> 4\u001b[0;31m                                       max_batch=400))\n\u001b[0m\u001b[1;32m      5\u001b[0m print('Test: ', opt.loss_and_accuracy((x_test, y_test), inference=True,\n\u001b[1;32m      6\u001b[0m                                      max_batch=400))\n",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36mloss_and_accuracy\u001b[0;34m(self, data, inference, max_batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             lossval, accval = self.sess.run(\n\u001b[0;32m--> 307\u001b[0;31m                 [self.network.loss, self.network.accuracy], feed)\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_ptr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0macc_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_ptr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print('Train: ', opt.loss_and_accuracy((x_train, y_train), inference=True,\n",
        "                                       max_batch=400))\n",
        "print('Valid: ', opt.loss_and_accuracy((x_validate, y_validate), inference=True,\n",
        "                                      max_batch=400))\n",
        "print('Test: ', opt.loss_and_accuracy((x_test, y_test), inference=True,\n",
        "                                     max_batch=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Xu8VTKgudL"
      },
      "outputs": [],
      "source": [
        "# Considera los maximos valores entre todas las epocas\n",
        "best_epoch = np.argmax(losses_and_accs_valid[:,1]) + 1\n",
        "print('Best epoch: ', best_epoch)\n",
        "print('Train acc: ', losses_and_accs_train[best_epoch-1, 1])\n",
        "print('Valid acc: ', losses_and_accs_valid[best_epoch-1, 1])\n",
        "print('Test acc: ', losses_and_accs_test[best_epoch-1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCQS8BiVgzwl"
      },
      "outputs": [],
      "source": [
        "# Save data\n",
        "losses_and_accs = np.concatenate(\n",
        "    [np.asarray(losses_and_accs_train),\n",
        "     np.asarray(losses_and_accs_valid),\n",
        "     np.asarray(losses_and_accs_test)], axis=1)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n",
        "ax1.semilogy(losses_and_accs[:,0], '-o', label='Train loss')\n",
        "ax1.semilogy(losses_and_accs[:,2], '-o', label='Valid loss')\n",
        "ax1.semilogy(losses_and_accs[:,4], '-o', label='Test loss')\n",
        "\n",
        "ax2.plot(losses_and_accs[:,1], '-o', label='Train acc')\n",
        "ax2.plot(losses_and_accs[:,3], '-o', label='Valid acc')\n",
        "ax2.plot(losses_and_accs[:,5], '-o', label='Test acc')\n",
        "\n",
        "for ax in [ax1,ax2]:\n",
        "    ax.legend()\n",
        "\n",
        "ax2.set_ylim(0.1,1)\n",
        "    \n",
        "print('Final results: ', losses_and_accs[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}